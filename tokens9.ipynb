{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Otter2022/Oil_Marketing_Campaign/blob/main/tokens9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3GxvCkOFUTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62538ff9-f026-47f6-e355-bee5e4966c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Dr', 'Doctor', 'Mr', 'Mrs', 'Ms', 'Miss', 'Prof', 'Professor', 'Sir', 'Madam', 'Dame', 'Lord', 'Lady', 'Rev', 'Reverend', 'Hon', 'Honorable', 'Capt', 'Captain', 'Major', 'Col', 'Colonel', 'Gen', 'General', 'Adm', 'Admiral', 'Sgt', 'Sergeant', 'Lt', 'Lieutenant', 'Chief', 'Officer', 'Judge', 'Magistrate', 'Sen', 'Senator', 'Rep', 'Representative', 'Amb', 'Ambassador', 'Pres', 'President', 'VP', 'Vice President', 'Gov', 'Governor', 'Mayor']\n",
            "['Jr', 'Sr', 'Esq', 'Esquire', 'PhD', 'MD', 'DDS', 'DVM', 'JD', 'CPA', 'RN', 'LPN', 'PE', 'Ret']\n",
            "['V', 'IV', 'III', 'II', 'I']\n",
            "{'FAM': 'Family', 'TR': 'Trust', 'REV': 'Revocable'}\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "#tokens9\n",
        "import csv\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "source_data = '/content/drive/MyDrive/ColabNotebooks/FinalProject_AI/Source_Data/'\n",
        "source_data_output = '/content/drive/MyDrive/ColabNotebooks/FinalProject_AI/Source_Data_Output/'\n",
        "source_code = '/content/drive/MyDrive/ColabNotebooks/FinalProject_AI/Source_Code/'\n",
        "admin = '/content/drive/MyDrive/ColabNotebooks/FinalProject_AI/Admin/'\n",
        "backups = '/content/drive/MyDrive/ColabNotebooks/FinalProject_AI/Backups/'\n",
        "\n",
        "#The lists contained in the marketing_compaign_oil.py file serves as a header for name pre-processing\n",
        "#contains exclude, compiled_patterns for exclude, fix_abbreviations, honorifics, suffixes, suffixes_roman, etc.\n",
        "marketing_campaign_oil = source_code+'marketing_campaign_oil.py'\n",
        "with open(marketing_campaign_oil, 'r') as f:\n",
        "    exec(f.read())\n",
        "    f.close()\n",
        "\n",
        "input_file = source_data + 'tokens9_4_CSV.csv'\n",
        "#input_file = basepath + 'tokens2_test_csv.csv'\n",
        "output_file = source_data_output + 'tokens9_formatted_output.csv'\n",
        "\n",
        "#test the reading of the marketing_campaign_oil package\n",
        "print(honorifics)\n",
        "print(suffixes)\n",
        "print(suffixes_roman)\n",
        "print(fix_abbreviations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_excluded(input_string):\n",
        "    # Check if any compiled pattern matches the input string\n",
        "    #I want to print the input_strings that are excluded\n",
        "    for pattern in compiled_patterns:\n",
        "        if pattern.match(input_string):\n",
        "            print(f\"Excluded: {input_string}\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def abbreviate(token):\n",
        "  if token in fix_abbreviations.keys():\n",
        "    return fix_abbreviations[token]\n",
        "  else:\n",
        "    return token\n",
        "\n",
        "\n",
        "def rearrange_tokens(name):\n",
        "    # Split the name into tokens\n",
        "    tokens = name.split()\n",
        "\n",
        "    # Exclude if any token for the owner name is in the exclude list\n",
        "    for token in tokens:\n",
        "        if is_excluded(token):\n",
        "            print(f\"Excluded: {name}\")\n",
        "            return \"Miss\"\n",
        "\n",
        "    # Process each token to fix abbreviations\n",
        "    tokens = [abbreviate(token.strip()) for token in tokens]\n",
        "    clean_tokens = [token.replace('\\ufeff', '') for token in tokens]\n",
        "\n",
        "    tokens = [token.lower() for token in clean_tokens]\n",
        "    names = []\n",
        "    special_tokens_honorifics = []\n",
        "    special_tokens_suffixes = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in honorifics:\n",
        "            special_tokens_honorifics.append(token)\n",
        "        elif token in suffixes_roman or token in suffixes:\n",
        "            special_tokens_suffixes.append(token)\n",
        "        else:\n",
        "            names.append(token)\n",
        "\n",
        "    rearranged = special_tokens_honorifics + names + special_tokens_suffixes\n",
        "\n",
        "    # Capitalize tokens that are not in honorifics or suffixes\n",
        "    capitalized_tokens = [token.capitalize() if token not in honorifics and token not in suffixes and token not in suffixes_roman else token for token in rearranged]\n",
        "    #separate capitalized_tokens like this: token1, token2,.... = capitalized_tokens\n",
        "    token1, token2, token3, token4, token5, token6, token7, token8, token9 = capitalized_tokens\n",
        "\n",
        "\n",
        "    # Rearrange the tokens based on the rules\n",
        "    if token1 in honorifics:\n",
        "        return f\"{token1} {token3} {token2} {token4} {token5} {token6} {token7} {token8} {token9}\"\n",
        "    elif token3 in suffixes or token3 in suffixes_roman:\n",
        "        return f\"{token1} {token2} {token3} {token4} {token5} {token6} {token7} {token8} {token9}\"\n",
        "    elif token2 == \"Trust\":\n",
        "        return f\"{token1} {token3} {token2} {token4} {token5} {token6} {token7} {token8} {token9}\"\n",
        "    elif token3 == \"Trust\":#do nothing\n",
        "        return f\"{token1} {token2} {token3} {token4} {token5} {token6} {token7} {token8} {token9}\"\n",
        "    elif token2 == \"Family\":\n",
        "        return f\"{token1} {token3} {token2} {token4}{token5} {token6} {token7} {token8} {token9}\"\n",
        "    elif token3 == \"Family\":\n",
        "        return f\"{token1} {token3} {token2} {token4} {token5} {token6} {token7} {token8} {token9}\"\n",
        "    elif token1.isalpha() and token2.isalpha() and token3.isalpha():\n",
        "        return f\"{token2} {token3} {token1} {token4} {token5} {token6} {token7} {token8} {token9}\"\n",
        "    else: #Smith A J, only rearrange first 3 tokens\n",
        "        return f\"{token2} {token3} {token1} {token4} {token5} {token6} {token7} {token8} {token9}\"\n",
        "    return \"Miss\"\n",
        "\n",
        "    #return \" \".join(capitalized_tokens)\n",
        "\n",
        "\n",
        "\n",
        "def process_names(input_file, output_file):\n",
        "    with open(input_file, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        names = [row[0] for row in reader]\n",
        "\n",
        "    names = [name.strip() for name in names]\n",
        "\n",
        "    formatted_names = [(name, rearrange_tokens(name)) for name in names]\n",
        "    misses = 0\n",
        "    total = len(formatted_names)\n",
        "    print(f\"Total: {total}\")\n",
        "\n",
        "    try:\n",
        "        with open(output_file, 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            for original, rearranged in formatted_names:\n",
        "                if rearranged == 'Miss':\n",
        "                    misses += 1\n",
        "                else:\n",
        "                    writer.writerow([original, rearranged])\n",
        "    except TypeError:\n",
        "        with open(output_file, 'w') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            for original, rearranged in formatted_names:\n",
        "                if rearranged == 'Miss':\n",
        "                    misses += 1\n",
        "                else:\n",
        "                    writer.writerow([original, rearranged])\n",
        "\n",
        "    return misses, total\n",
        "\n"
      ],
      "metadata": {
        "id": "XYNx4_omFcto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    misses,total = process_names(input_file, output_file)\n",
        "    print(f\"Miss percentage {(float(misses)/float(total))*100.00}%.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkDgnu8nFcEL",
        "outputId": "158b50c9-8c31-493e-f1bf-6db6d1ba55e0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total: 4\n",
            "Miss percentage 0.0%.\n"
          ]
        }
      ]
    }
  ]
}